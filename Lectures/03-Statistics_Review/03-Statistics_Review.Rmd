---
title: "Statistics Review II"
subtitle: "EC 320: Introduction to Econometrics"
author: "Tami Ren"
date: "Summer 2022"
output:
   xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---
class: inverse, middle

```{r, SETUP, include = F}
options(htmltools.dir.version = FALSE)

library(pacman)
p_load(xaringan,ggthemes, viridis, knitr, dslabs, gapminder, extrafont, Ecdat, wooldridge, tidyverse, magrittr,svglite,renderthis,xaringanthemer,parallel,latex2exp,dslabs,ggforce)


knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 4,
  fig.width = 6,
  cache = TRUE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)

style_duo_accent(
  primary_color = "#1F4257",
  secondary_color = "#F97B64",
  inverse_header_color = "#FFFFFF"
)
# Notes directory
dir_slides <- "/Users/tami/Dropbox (University of Oregon)/EC320--Ren--2022/EC320/Lectures/03-Introduction"
# Knitr options




```


---
class: inverse, middle

# Statistics Review

---
# Overview

__Goal:__ Learn about a population.

- In particular, learn about an unknown population _parameter_.

__Challenge:__ Usually cannot access information about the entire population.

__Solution:__ Sample from the population and estimate the parameter.

- Draw $n$ observations from the population, then use an estimator.

---
# Sampling

There are myriad ways to produce a sample,<sup>*</sup> but we will restrict our attention to __simple random sampling__, where

1. Each observation is a random variable.

2. The $n$ random variables are independent.

3. Life becomes much simpler for the econometrician.

.footnote[
<sup>*</sup> Only a subset of these can help produce reliable statistics.
]

---
# Estimators

An __estimator__ is a rule (or formula) for estimating an unknown population parameter given a sample of data.

--

- Each observation in the sample is a random variable.

--

- An estimator is a combination of random variables $\implies$ it is a random variable.

__Example:__ Sample mean

$$
\bar{X} = \dfrac{1}{n} \sum_{i=1}^n X_i
$$

- $\bar{X}$ is an estimator for the population mean $\mu$.

- Given a sample, $\bar{X}$ yields an __estimate__ $\bar{x}$ or $\hat{\mu}$, a specific number.

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

```{r, gen dataset, include = F, cache = T}
# Set population and sample sizes
n_p <- 100
n_s <- 10
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 2, sd = 20),
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Means
m0 <- mean(pop_df$x)
m1 <- mean(subset(pop_df$x, pop_df$s1 == T))
m2 <- mean(subset(pop_df$x, pop_df$s2 == T))
m3 <- mean(subset(pop_df$x, pop_df$s3 == T))
# Simulation
set.seed(12468)
sim_df <- mclapply(mc.cores = 1, X = 1:1e4, FUN = function(x, size = n_s) {
  pop_df %>% 
    sample_n(size = size) %>% 
    summarize(mu_hat = mean(x))
}) %>% do.call(rbind, .) %>% as_tibble()
```

.pull-left[

```{r, pop1, echo = F, fig.fullwidth = F}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "darkslategray", size = 10) +
theme_minimal()
```

.center[**Population**]

]

--

.pull-right[

```{r, mean1, echo = F, fig.fullwidth = F}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "darkslategray", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "darkslategray") +
  theme_minimal()
```

.center[**Population relationship**]
$\mu = `r round(m0, 2)`$

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

```{R, sample1, echo = F, fig.fullwidth = F}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_minimal() + theme(legend.position="none")
```

.center[**Sample 1:** 10 random individuals]

]

--

.pull-right[

```{r, sample1 mean, echo = F, fig.fullwidth = F}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "darkslategray", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "darkslategray") +
  geom_histogram(data = subset(pop_df, s1 == T), aes(x), fill = "dark red", alpha = 0.50) +
  geom_vline(xintercept = m1, size = 2, color = "dark red") +
  theme_minimal()
```

.center[

**Population relationship** - $\mu = `r round(m0, 2)`$

**Sample relationship** - $\hat{\mu} = `r round(m1, 2)`$

]

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

```{r, sample2, echo = F, fig.fullwidth = F}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_minimal() + theme(legend.position="none")
```

.center[**Sample 2:** 10 random individuals]

]

--

.pull-right[

```{r, sample2 mean, echo = F, fig.fullwidth = F}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "darkslategray", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "darkslategray") +
  geom_histogram(data = subset(pop_df, s2 == T), aes(x), fill = "dark red", alpha = 0.50) +
  geom_vline(xintercept = m2, size = 2, color = "dark red") +
  theme_minimal()
```

.center[

**Population relationship** - $\mu = `r round(m0, 2)`$

**Sample relationship**- $\hat{\mu} = `r round(m2, 2)`$

]

]

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[

```{r, sample3, echo = F, fig.fullwidth = F}
ggplot(data = pop_df, aes(x = row, y = col, shape = s3)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_minimal()+ theme(legend.position="none")
```

.center[**Sample 3:** 10 random individuals]

]

--

.pull-right[

```{r, sample3 mean, echo = F, fig.fullwidth = F}
ggplot() +
  geom_histogram(data = pop_df, aes(x), fill = "darkslategray", alpha = 0.50) +
  geom_vline(xintercept = m0, size = 2, color = "darkslategray") +
  geom_histogram(data = subset(pop_df, s3 == T), aes(x), fill = "dark red", alpha = 0.50) +
  geom_vline(xintercept = m3, size = 2, color = "dark red") +
  theme_minimal()
```

.center[

**Population relationship** - $\mu = `r round(m0, 2)`$

**Sample relationship** - $\hat{\mu} = `r round(m3, 2)`$

]

]

---
class: clear-slide, middle

Let's repeat this **10,000 times** and then plot the estimates.

(This exercise is called a Monte Carlo simulation.)

---
class: clear-slide, middle

```{r, simulation, echo = F}
ggplot() +
  geom_histogram(data = sim_df, aes(mu_hat), fill = "dark red", alpha = 0.75) +
  geom_vline(xintercept = m0, size = 2, color = "darkslategray") +
  scale_x_continuous(breaks = m0, labels = TeX("$\\mu$")) +
  xlab(TeX("$\\hat{\\mu}$")) +
  theme(axis.text.x = element_text(size = 20),
      axis.text.y = element_blank(),
      rect = element_blank(),
      axis.title.y = element_blank(),
      axis.title.x = element_text(size = 20, hjust = 1, color = "dark gray"),
      line = element_blank())
```

---
# Population *vs.* Sample

**Question:** Why do we care about *population vs. sample*?

.pull-left[
```{r, simulation2, echo = F}
ggplot() +
  geom_histogram(data = sim_df, aes(mu_hat), fill = "dark red", alpha = 0.75) +
  geom_vline(xintercept = m0, size = 2, color = "darkslategray") +
  scale_x_continuous(breaks = m0, labels = TeX("$\\mu$")) +
  xlab(TeX("$\\hat{\\mu}$")) +
  theme(axis.text.x = element_text(size = 20),
      axis.text.y = element_blank(),
      rect = element_blank(),
      axis.title.y = element_blank(),
      axis.title.x = element_text(size = 20, hjust = 1, color = "dark gray"),
      line = element_blank())
```
]

.pull-right[

- On average, the mean of the samples are close to the population mean.

- But some individual samples can miss the mark.

- The difference between individual samples and the population creates __uncertainty__. 

]

???


**Question:** Why do we care about *population vs. sample*?

**Answer:** Uncertainty matters.

- $\hat{\mu}$ is a random variable that depends on the sample.

- In practice, we don't know whether our sample is similar to the population or not. 

- Individual samples may have means that differ greatly from the population.

- We will have to keep track of this uncertainty.


---
class: inverse, middle

# Estimators

---
# Properties of Estimators

An **estimator** is a general rule for estimating an unknown population parameter, given a sample of data. How do we decide which estimator to use?

```{r, competing pdfs, echo = F, fig.height = 3, fig.width = 4.5}
# Generate data for densities' polygons
d1 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 1, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d2 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dunif(x, min = -2.5, max = 1.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d3 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
# Plot them
ggplot() +
geom_polygon(data = d1, aes(x, y), alpha = 0.8, fill = "orange") +
geom_polygon(data = d2, aes(x, y), alpha = 0.65, fill = "dark red") +
geom_polygon(data = d3, aes(x, y), alpha = 0.6, fill = "darkslategray") +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\mu$")) +
theme_minimal()+
theme(axis.text.x = element_text(size = 20),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

---
# Unbiasedness

An estimator is **unbiased** if the expected value of the estimator is equal to the population parameter  

$$ \mathop{\text{Bias}_\mu} \left( \hat{\mu} \right) = \mathop{\mathbb{E}}\left[ \hat{\mu} \right] - \mu $$



---
# Unbiasedness


.pull-left[

**Unbiased estimator:** $\mathop{\mathbb{E}}\left[ \hat{\mu} \right] = \mu$

```{r, unbiased pdf, echo = F}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "dark red", alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\mu$")) +
theme_minimal() +
theme(axis.text.x = element_text(size = 40),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

]

--

.pull-right[

**Biased estimator:** $\mathop{\mathbb{E}}\left[ \hat{\mu} \right] \neq \mu$

```{r, biased pdf, echo = F}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = "darkslategray", alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\mu$")) +
theme_minimal() +
theme(axis.text.x = element_text(size = 40),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

]

---
# Efficiency

We also care about estimates that have lower variance (more **efficient**). Low variance estimators produce estimates closer to the mean in each sample. 


$$ \mathop{\text{Var}} \left( \hat{\mu} \right) = \mathop{\mathbb{E}}\left[ \left( \hat{\mu} - \mathop{\mathbb{E}}\left[ \hat{\mu} \right] \right)^2 \right] $$


---
# Efficiency


```{r, variance pdf, echo = F, fig.height = 3}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = "dark red", alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\mu$")) +
theme_minimal() +
theme(axis.text.x = element_text(size = 20),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

---
# The Bias-Variance Tradeoff


```{r, variance bias, echo = F, fig.height = 3}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0.3, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = "dark red", alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\mu$")) +
theme_minimal() +
theme(axis.text.x = element_text(size = 20),
      axis.text.y = element_blank(),
      axis.title = element_blank(),
      line = element_blank())
```

---
# Unbiased Estimators

__Sample mean__ 

$$\bar{X} = \frac{1}{n}\Sigma^n_{i=1} X_i$$
- __Sample variance__ 

$$S_{X}^2 = \dfrac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2.$$
---
# Unbiased Estimators
- __Sample covariance__ 

$$S_{XY} = \dfrac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y}).$$

- __Sample correlation__ 

$$r_{XY} = \dfrac{S_{XY}}{\sqrt{S_X^2} \sqrt{S_Y^2}}.$$

---
class: inverse, middle

# Hypothesis Testing

---
# Hypothesis Testing 

Given What do we make of an estimate of the population mean?

- Is it meaningfully different than existing evidence on the population mean?
- Is is _statistically distinguishable_ from previously hypothesized values of the population mean?
- Is the estimate extreme enough to update our prior beliefs about the population mean?

We can conduct statistical tests to address these questions.

---
# Hypothesis Testing

__Null hypothesis (H.sub[0]):__ $\mu = \mu_0$

__Alternative hypothesis (H.sub[1]):__ $\mu \neq \mu_0$

--

There are four possible outcomes of our test:

1. We __fail to reject__ the null hypothesis and the null is true.

2. We __reject__ the null hypothesis and the null is false.

3. We __reject__ the null hypothesis, but the null is actually true (**Type I error**).

4. We __fail to reject__ the null hypothesis, but the null is actually false (**Type II error**).


---
# Hypothesis Testing

$\hat{\mu}$ is random: it could be anything, even if $\mu = \mu_0$ is true.

- But if $\mu = 0$ is true, then $\hat{\mu}$ is unlikely to take values far from zero.

- As the variance of $\hat{\mu}$ shrinks, we are even less likely to observe "extreme" values of $\hat{\mu}$.

- For now, we'll assume that the variable of interest $X$ is normally distributed with mean $\mu$ and standard deviation $\sigma^2$.

---
# Hypothesis Testing

Reject H.sub[0] if $\hat{\mu}$ lies in the .hi[rejection region].

```{r, echo = F, fig.height = 3.75}
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dnorm(seq(-4,4, by = 0.01))
)
crit <- qnorm(c(.025,.975))
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0), breaks = c(-1.96, 0, 1.96), labels = c(TeX("$\\mu_0 - 1.96 \\, s.d.$"), TeX("$\\mu_0$"), TeX("$\\mu_0 + 1.96 \\, sd$"))) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  geom_polygon(data = tail_left, aes(x=x, y=y), fill = "dark red") +
  geom_polygon(data = tail_right, aes(x=x, y=y), fill = "dark red") +
  geom_polygon(data = df %>% filter(x <= qnorm(1 - 0.975) & x >= qnorm(0.975)), aes(x, y), fill = "dark red") +
  geom_vline(xintercept = qnorm(0.975), size = 0.35, linetype = "dashed", color = "dark gray") +
  geom_vline(xintercept = qnorm(1 - 0.975), size = 0.35, linetype = "dashed", color = "dark gray") +
  theme_minimal()+
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

- The area of the rejection region is defined by the **significance level** of the test.
- In a 5% test, the area is 0.05. 
- Significance level .mono[=] tolerance for Type I error.

---
# Hypothesis Testing

Reject H.sub[0] if $\left| z \right| =\left| \dfrac{\hat{\mu} - \mu_0}{\mathop{\text{sd}}(\hat{\mu})} \right| > 1.96$.

```{r, echo = F, fig.height = 3.75}
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dnorm(seq(-4,4, by = 0.01))
)
crit <- qnorm(c(.025,.975))
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0), breaks = c(-1.96, 0, 1.96), labels = c(TeX("$\\mu_0 - 1.96 \\, s.d.$"), TeX("$\\mu_0$"), TeX("$\\mu_0 + 1.96 \\, sd$"))) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_polygon(data = df, aes(x, y), fill = "grey85") +
  geom_polygon(data = tail_left, aes(x=x, y=y), fill = "dark red") +
  geom_polygon(data = tail_right, aes(x=x, y=y), fill = "dark red") +
  geom_polygon(data = df %>% filter(x <= qnorm(1 - 0.975) & x >= qnorm(0.975)), aes(x, y), fill = "dark red") +
  geom_vline(xintercept = qnorm(0.975), size = 0.35, linetype = "dashed", color = "dark gray") +
  geom_vline(xintercept = qnorm(1 - 0.975), size = 0.35, linetype = "dashed", color = "dark gray") +
  theme_minimal()+
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

What happens to $z$ as $\left| \hat{\mu} - \mu_0 \right|$ increases? 

What happens to $z$ as $\mathop{\text{sd}}(\hat{\mu})$ increases?

---
# Hypothesis Testing

The formula for the $z$ statistic assumes that we know $\mathop{\text{sd}}(\hat{\mu})$.

- In practice, we don't know $\mathop{\text{sd}}(\hat{\mu})$, so we have to estimate it.

--

If the variance of $X$ is $\sigma^2$, then 

$$\sigma^2_{\hat{\mu}} = \dfrac{\sigma^2}{n}.$$

- We can estimate $\sigma^2$ with the sample variance $S_{X}^2$.

--

The sample variance of the sample mean is
 
$$S_{\hat{\mu}}^2 = \dfrac{1}{n(n-1)} \sum_{i=1}^n (X_i - \bar{X})^2.$$

---
# Hypothesis Testing

The .hi[standard error] of $\hat{\mu}$ is the square root of $S_{\hat{\mu}}^2$:

$$\mathop{\text{SE}}(\hat{\mu}) = \sqrt{ \dfrac{1}{n(n-1)} \sum_{i=1}^n (X_i - \bar{X})^2}.$$

- Standard error = sample standard deviation of an estimator.

--

When we use $\mathop{\text{SE}}(\hat{\mu})$ in place of $\mathop{\text{sd}}(\hat{\mu})$, the $z$ statistic becomes a $t$ statistic:

$$t = \dfrac{\hat{\mu} - \mu_0}{\mathop{\text{SE}}(\hat{\mu})}.$$

- Unlike the standard deviation of $\hat{\mu}$, $\mathop{\text{SE}}(\hat{\mu})$ varies from sample to sample.
- **Consequence:** $t$ statistics do not necessarily have a normal distribution.

---
# Hypothesis Testing

## .hi-green[Normal distribution] vs. .hi-purple[t distribution]

- A normal distribution has the same shape for any sample size.
- The shape of the t distribution depends the **degrees of freedom**.

```{r, echo = F, fig.height = 3.5}
n <- 5
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), n),
    y_norm = dnorm(seq(-4,4, by = 0.01))
)
crit <- qt(c(.025,.975), n)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_line(data = df, aes(x, y), color = "#9370DB", size = 1) +
  geom_line(data = df, aes(x, y_norm), color = "#007935", size = 1) +
  # geom_polygon(data = tail_left, aes(x=x, y=y), fill = "dark red") +
  # geom_polygon(data = tail_right, aes(x=x, y=y), fill = "dark red") +
  # geom_polygon(data = df %>% filter(x <= qt(1 - 0.975, n) & x >= qt(0.975, n)), aes(x, y), fill = "dark red") +
  geom_vline(xintercept = qt(0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = qt(1 - 0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = -1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  geom_vline(xintercept = 1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  theme_minimal() +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

- Degrees of freedom .mono[=] 5.

---
count: false

# Hypothesis Testing

## .hi-green[Normal distribution] vs. .hi-purple[t distribution]

- A normal distribution has the same shape for any sample size.
- The shape of the t distribution depends the **degrees of freedom**.

```{r, echo = F, fig.height = 3.5}
n <- 50
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), n),
    y_norm = dnorm(seq(-4,4, by = 0.01))
)
crit <- qt(c(.025,.975), n)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_line(data = df, aes(x, y), color = "#9370DB", size = 1) +
  geom_line(data = df, aes(x, y_norm), color = "#007935", size = 1) +
  # geom_polygon(data = tail_left, aes(x=x, y=y), fill = "dark red") +
  # geom_polygon(data = tail_right, aes(x=x, y=y), fill = "dark red") +
  # geom_polygon(data = df %>% filter(x <= qt(1 - 0.975, n) & x >= qt(0.975, n)), aes(x, y), fill = "dark red") +
  geom_vline(xintercept = qt(0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = qt(1 - 0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = -1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  geom_vline(xintercept = 1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  theme_minimal() +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

- Degrees of freedom .mono[=] 50.

---
count: false

# Hypothesis Testing

## .hi-green[Normal distribution] vs. .hi-purple[t distribution]

- A normal distribution has the same shape for any sample size.
- The shape of the t distribution depends the **degrees of freedom**.

```{r, echo = F, fig.height = 3.5}
n <- 500
df <- tibble(
    x = seq(-4,4, by = 0.01),
    y = dt(seq(-4,4, by = 0.01), n),
    y_norm = dnorm(seq(-4,4, by = 0.01))
)
crit <- qt(c(.025,.975), n)
tail_left <- rbind(c(crit[1],0), subset(df, x < crit[1]))
tail_right <- rbind(c(crit[2],0), subset(df, x > crit[2]), c(3,0))
ggplot() +
  scale_x_continuous(limits = c(-4, 4), expand=c(0,0)) +
  scale_y_continuous(limits = c(0, 0.5), expand=c(0,0), breaks = c(0, 0.5), labels = c(0, 0.5)) +
  geom_line(data = df, aes(x, y), color = "#9370DB", size = 1) +
  geom_line(data = df, aes(x, y_norm), color = "#007935", size = 1) +
  # geom_polygon(data = tail_left, aes(x=x, y=y), fill = "dark red") +
  # geom_polygon(data = tail_right, aes(x=x, y=y), fill = "dark red") +
  # geom_polygon(data = df %>% filter(x <= qt(1 - 0.975, n) & x >= qt(0.975, n)), aes(x, y), fill = "dark red") +
  geom_vline(xintercept = qt(0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = qt(1 - 0.975, n), size = 0.35, linetype = "dashed", color = "#9370DB") +
  geom_vline(xintercept = -1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  geom_vline(xintercept = 1.96, size = 0.35, linetype = "dashed", color = "#007935") +
  theme_minimal() +
  xlab("") + 
  ylab("") + theme(axis.text.y = element_blank(), axis.line.y = element_blank())
```

- Degrees of freedom .mono[=] 500.

---
# Hypothesis Testing

## **t Tests** (two-sided)

To conduct a t test, compare the $t$ statistic to the appropriate .hi[critical value] of the t distribution.

- To find the critical value in a t table, we need the degrees of freedom and the significance level $\alpha$.

Reject H.sub[0] at the $\alpha \cdot 100$-percent level if 

$$\left| t \right| = \left| \dfrac{\hat{\mu} - \mu_0}{\mathop{\text{SE}}(\hat{\mu})} \right| > t_\text{crit}.$$

---
# Hypothesis Testing

## On Your Own

As the term progresses, we will encounter additional flavors of hypothesis testing and other related concepts.

You may find it helpful to review the following topics from Math 243:

- Confidence intervals
- One-sided $t$ tests
- $p$ values

---
class: inverse, middle

# Data and the .mono[tidyverse]

---
# Data

## Experimental data

Data generated in controlled, laboratory settings.

--

Ideal for __causal identification__, but difficult to obtain in the social sciences.

- Intractable logistical problems
- Too expensive
- Morally repugnant

--

Experiments outside the lab: __randomized control trials__ and __A/B testing__.

---
# Data

## Observational data

Data generated in non-experimental settings.

--

- Surveys
- Censuses
- Administrative records
- Environmental data
- Financial and sales transactions
- Social media

--

Mainstay of economic research, but __poses challenges__ to causal identification.


---
# Cross Sectional Data

.hi-purple[Sample of individuals from a population at a point in time.]

Ideally, collected using __random sampling__.

- Random sampling .mono[+] sufficient sample size .mono[=] representative sample.

- Random sampling simplifies data analysis, but non-random samples are common (and difficult to work with).

Used extensively in applied microeconomics.<sup>*</sup>

__Main focus of this course.__

.footnote[
<sup>*</sup> Applied microeconomics .mono[=] Labor, health, education, public finance, development, industrial organization, and urban economics.
]


---
# Time Series Data

.hi-purple[Observations of variables over time.]

- Quarterly US GDP
- Annual US infant mortality rates
- Daily Amazon stock prices

Complication: Observations are not independent draws.

- GDP this quarter highly related to GDP last quarter.

Used extensively in empirical macroeconomics.

Requires more-advanced methods (EC 421 and EC 422).

---
# Pooled Cross Sectional Data

.hi-purple[Cross sections from different points in time.]

Useful for studying policy changes and relationships that change over time.

Requires more-advanced methods (EC 421 and many 400-level applied micro classes).

---
# Panel or Longitudinal Data

.hi-purple[Time series for each cross-sectional unit.]

- Example: daily attendance data for a sample of students.

Difficult to collect, but useful for causal identification.

- Can control for _unobserved_ characteristics.

Requires more-advanced methods (EC 421 and many 400-level applied micro classes).


```{r,download}

to_pdf("03-Statistics_Review.RMD")

```