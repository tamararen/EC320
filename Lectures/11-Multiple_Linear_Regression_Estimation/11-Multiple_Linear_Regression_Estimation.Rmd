---
title: "Multiple Linear Regression: Estimation"
subtitle: "EC 320: Introduction to Econometrics"
author: "Tami Ren"
date: "Summer 2022"
output:
   xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(ggthemes, viridis, knitr, extrafont, tidyverse, magrittr, wooldridge, dagitty, ggdag, latex2exp, parallel, broom, kableExtra, ggforce, furrr, xaringanthemer)

dir_slides <- "~/Lectures/11-Multiple_Linear_Regression_Estimation/"


# Define colors
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
met_slate <- "#23373b" # metropolis font color
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  #dpi = 300,
  #cache = T,
  warning = F,
  message = F
)  

#Theme style
style_duo_accent(
  primary_color = "#1F4257",
  secondary_color = "#F97B64",
  inverse_header_color = "#FFFFFF"
)
# election data
election <- read_csv("election_2016.csv") %>% 
  mutate(trump_pct = trump/totalvotes*100,
         clinton_pct = clinton/totalvotes*100,
         trump_margin = trump_pct - clinton_pct,
         nonwhite = 100 - white)
```

---
class: inverse, middle

# Multiple Linear Regression

---
# Multiple Linear Regression

## More explanatory variables

**Simple linear regression** features one .pink[outcome variable] and one .purple[explanatory variable]:

$$\color{#e64173}{Y_i} = \beta_0 + \beta_1 \color{#9370DB}{X_i} + u_i.$$

**Multiple linear regression** features one .pink[outcome variable] and multiple .purple[explanatory variables]:

$$\color{#e64173}{Y_i} = \beta_0 + \beta_1 \color{#9370DB}{X_{1i}} + \beta_2 \color{#9370DB}{X_{2i}} + \cdots + \beta_{k} \color{#9370DB}{X_{ki}} + u_i.$$

--

**Why?**

--

- Better explain the variation in $Y$.
- Improve predictions.
- Avoid bias.

---
# Multiple Linear Regression

```{R, echo = F}
# Colors (order: x1, x2, x3, y)
venn_colors <- c(purple, red, "grey60", orange)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5,    1.5,   -1.0),
  y  = c( 0.0,   -2.5,   -1.8,    2.0),
  r  = c( 1.9,    1.5,    1.5,    1.3),
  l  = c( "Y", "X[1]", "X[2]", "X[3]"),
  xl = c( 0.0,   -0.5,    1.6,   -1.0),
  yl = c( 0.0,   -2.5,   -1.9,    2.2)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```

---
# OLS Estimation

As was the case with simple linear regressions, OLS minimizes the sum of squared residuals (RSS).

However, residuals are now defined as

$$\hat{u}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_{1i} - \hat{\beta}_2 X_{2i} - \cdots - \hat{\beta}_{k} X_{ki}.$$

--

To obtain estimates, take partial derivatives of RSS with respect to each $\hat{\beta}$, set each derivative equal to zero, and solve the system of $k+1$ equations.

- Without matrices, the algebra is difficult. For the remainder of this course, we will let .mono[R] do the work for us.

---
# Coefficient Interpretation

**Model**

$$\color{}{Y_i} = \beta_0 + \beta_1 \color{}{X_{1i}} + \beta_2 \color{}{X_{2i}} + \cdots + \beta_{k} \color{}{X_{ki}} + u_i.$$

**Interpretation**

- The intercept $\hat{\beta}_0$ is the average value of $Y_i$ when all of the explanatory variables are equal to zero.
- Slope parameters $\hat{\beta}_1, \dots, \hat{\beta}_{k}$ give us the change in $Y_i$ from a one-unit change in $X_j$, holding the other $X$ variables constant. 

---
# Algebraic Properties of OLS

The OLS first-order conditions yield the same properties as before.

1. Residuals sum to zero: $\sum_{i=1}^n \hat{u_i} = 0$.

2. The sample covariance between the independent variables and the residuals is zero.

3. The point $(\bar{X_1}, \bar{X_2}, \dots, \bar{X_k}, \bar{Y})$ is always on the fitted regression "line."

---
# Goodness of Fit

Fitted values are defined similarly:

$$\hat{Y_i} = \hat{\beta}_0 + \hat{\beta}_1 X_{1i} + \hat{\beta}_2 X_{2i} + \cdots + \hat{\beta}_{k} X_{ki}.$$

The formula for $R^2$ is the same as before:

$$R^2 =\frac{\sum(\hat{Y_i}-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}.$$

---
# Goodness of Fit

**Model 1:** $Y_i = \beta_0 + \beta_1 X_{1i} + u_i$.

**Model 2:** $Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + v_i$

--

.hi-green[True or false?]

Model 2 will yield a lower $R^2$ than Model 1.

- Hint: Think of $R^2$ as $R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}$.

---
# Goodness of Fit 

```{R, echo = F}
# Colors (order: x1, x2, x3, y)
venn_colors <- c(purple, orange)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5),
  y  = c( 0.0,   -2.5),
  r  = c( 1.9,    1.5),
  l  = c( "Y", "X[1]"),
  xl = c( 0.0,   -0.5),
  yl = c( 0.0,   -2.5)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
annotate(
  x = 1, y = 3,
  geom = "text", label = "Model 1", size = 9, color = met_slate, family = "Fira Sans", hjust = 0
) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```

---
# Goodness of Fit 

```{R, echo = F}
# Colors (order: x1, x2, x3, y)
venn_colors <- c(purple, red, orange)
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5, -1.0),
  y  = c( 0.0,   -2.5, 2.0),
  r  = c( 1.9,    1.5, 1.3),
  l  = c( "Y", "X[1]", "X[2]"),
  xl = c( 0.0,   -0.5, -1.0),
  yl = c( 0.0,   -2.5, 2.2)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
annotate(
  x = 1, y = 3,
  geom = "text", label = "Model 2", color = met_slate, size = 9, family = "Fira Sans", hjust = 0
) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```


---
# Goodness of Fit

**Problem:** As we add variables to our model, $R^2$ *mechanically* increases.

**One solution:** Penalize for the number of variables, _e.g._, adjusted $R^2$:

$$ \bar{R}^2 = 1 - \dfrac{\sum_i \left( Y_i - \hat{Y}_i \right)^2/(n-k-1)}{\sum_i \left( Y_i - \bar{Y} \right)^2/(n-1)} $$

*Note:* Adjusted $R^2$ need not be between 0 and 1.

---
# Goodness of Fit

## Example: 2016 Election

```{r}
lm(trump_margin ~ white, data = election) %>% glance()
```

--

```{r}
lm(trump_margin ~ white + poverty, data = election) %>% glance()
```
---
# OLS Assumptions 

Same as before, except for .pink[assumption 2]:

1. **Linearity:** The population relationship is linear in parameters with an additive error term.
2. .hi[No perfect collinearity:] .pink[No] $\color{#e64173}{X}$ .pink[variable is a perfect linear combination of the others.]
3. **Random Sampling:** We have a random sample from the population of interest.
4. **Exogeneity:** The $X$ variable is exogenous (*i.e.,* $\mathop{\mathbb{E}}\left( u|X \right) = 0$).
5. **Homoskedasticity:** The error term has the same variance for each value of the independent variable (*i.e.,* $\mathop{\text{Var}}(u|X) = \sigma^2$).
6. **Normality:** The population error term is normally distributed with mean zero and variance $\sigma^2$ (*i.e.,* $u \sim N(0,\sigma^2)$)

---
# Perfect Collinearity

## Example: 2016 Election

OLS cannot estimate parameters for .mono[white] and .mono[nonwhite] simultaneously.

- .mono[white] .mono[=] 100 .mono[-] .mono[nonwhite].

--

```{r}
lm(trump_margin ~ white + nonwhite, data = election) %>% tidy()
```

.mono[R] drops perfectly collinear variables for you.

---
# Multiple Linear Regression 

## Tradeoffs

There are tradeoffs to remember as we add/remove variables:

**Fewer variables**

- Generally explain less variation in $y$.
- Provide simple interpretations and visualizations (*parsimonious*).
- May need to worry about omitted-variable bias.

**More variables**

- More likely to find *spurious* relationships (statistically significant due to chance; do not reflect true, population-level relationships).
- More difficult to interpret the model.
- May still leave out important variables.

---
# Omitted Variables

```{R, venn2,  echo = F}
# Line types (order: x1, x2, x3, y)
venn_lines <- c("solid", "dotted", "solid")
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
annotate(
  x = 1, y = 3,
  geom = "text", label = "No Bias", color = met_slate, size = 9, family = "Fira Sans", hjust = 0
) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```

---
# Omitted Variables

```{R, echo = F}
# Colors (order: x1, x2, x3, y)
venn_lines <- c("solid", "dotted", "solid")
# Locations of circles
venn_df <- tibble(
  x  = c( 0.0,   -0.5, 1.5),
  y  = c( 0.0,   -2.5, -1.8),
  r  = c( 1.9,    1.5, 1.5),
  l  = c( "Y", "X[1]", "X[2]"),
  xl = c( 0.0,   -0.5, 1.6),
  yl = c( 0.0,   -2.5, -1.9)
)
# Venn
ggplot(data = venn_df, aes(x0 = x, y0 = y, r = r, fill = l, color = l)) +
geom_circle(aes(linetype = l), alpha = 0.3, size = 0.75) +
theme_void() +
theme(legend.position = "none") +
scale_fill_manual(values = venn_colors) +
scale_color_manual(values = venn_colors) +
scale_linetype_manual(values = venn_lines) +
geom_text(aes(x = xl, y = yl, label = l), size = 9, family = "Fira Sans", parse = T) +
annotate(
  x = 1, y = 3,
  geom = "text", label = "Bias", color = met_slate, size = 9, family = "Fira Sans", hjust = 0
) +
xlim(-3, 3) +
ylim(-4, 3.4) +
coord_equal()
```

---
# Omitted Variables

```{R, echo = F, escape = F}
tab <- data.frame(
  v1 = c("Intercept", "", "log(Spend)", "", "Lunch", ""),
  v2 = rbind(
    c(-84.84, -1.52, ""),
    c("(18.57)", "(2.18)", "")
  ) %>% as.vector(),
  v3 = rbind(
    c(-6.34, 11.34, -0.47),
    c("(15.00)", "(1.77)", "(0.01)")
  ) %>% as.vector()
) %>% kable(
  escape = F,
  col.names = c("Explanatory variable", "1", "2"),
  align = c("l", rep("c", 3)),
  caption = "Math Score"
) %>%
row_spec(1:6, color = met_slate) %>%
row_spec(seq(2,6,2), color = "#c2bebe") %>%
row_spec(1:6, extra_css = "line-height: 110%;") %>%
column_spec(1, color = "black", italic = T)
tab %>% column_spec(2, bold = T)
```

Data from 1823 elementary schools in Michigan

- *Math Score* is average fourth grade state math test scores.
- *log(Spend)* is the natural logarithm of spending per pupil.
- *Lunch* is the percentage of student eligible for free or reduced-price lunch.

---
count: false

# Omitted Variables

```{R, echo = F, escape = F}
tab %>% column_spec(3, bold = T)
```

Data from 1823 elementary schools in Michigan

- *Math Score* is average fourth grade state math test scores.
- *log(Spend)* is the natural logarithm of spending per pupil.
- *Lunch* is the percentage of student eligible for free or reduced-price lunch.

---
# Omitted-Variable Bias

**Model 1:** $Y_i = \beta_0 + \beta_1 X_{1i} + u_i$.

**Model 2:** $Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + v_i$

Estimating Model 1 (without $X_2$) yields .hi[omitted-variable bias:]

$$\color{#e64173}{\text{Bias} = \beta_2 \frac{\mathop{\text{Cov}}(X_{1i}, X_{2i})}{\mathop{\text{Var}}(X_{1i})}}.$$

--

The sign of the bias depends on

1. The correlation between $X_2$ and $Y$, _i.e._, $\beta_2$.

2. The correlation between $X_1$ and $X_2$, _i.e._, $\mathop{\text{Cov}}(X_{1i}, X_{2i})$.



